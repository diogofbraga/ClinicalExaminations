{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP - Computação Natural\n",
    "#### \"Predict whether a mammogram mass is benign or malignant\"\n",
    "\n",
    "1. BI-RADS assessment: 1 to 5 (ordinal)  \n",
    "2. Age: patient's age in years (integer)\n",
    "3. Shape: mass shape: round=1 oval=2 lobular=3 irregular=4 (nominal)\n",
    "4. Margin: mass margin: circumscribed=1 microlobulated=2 obscured=3 ill-defined=4 spiculated=5 (nominal)\n",
    "5. Density: mass density high=1 iso=2 low=3 fat-containing=4 (ordinal)\n",
    "6. Severity: benign=0 or malignant=1 (binominal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mammographic_masses.data.txt')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Convert missing data (indicated by a ?) into NaN and add the appropriate column names (BI_RADS, age, shape, margin, density, and severity) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace('?',np.nan)\n",
    "data.columns = ['BI_RADS','Age','Shape','Margin','Density','Severity']\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Drop BI_RADS column because it has no influence on the severity forecast **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['BI_RADS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Convert datatype 'object' to 'float64' **  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(float)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing missing values\n",
    "**First we get the missing values per feature.**\n",
    "\n",
    "*Lets check them out as well*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_feature = data.isnull().sum(axis=0)\n",
    "graph = missing_values_feature.drop(labels='Severity')\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(131)\n",
    "plt.bar(graph.axes[0].to_list(), graph.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We then develop a heatmap to give us some more information*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data.isnull(),yticklabels=False,cbar=False,cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Finally we can check the percentage of missing values per feature*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing = data.isnull().sum() * 100 / len(data)\n",
    "missing_value_df = pd.DataFrame({'percent_missing': percent_missing})\n",
    "print(missing_value_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After analysing the columns, we should have a look at the rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_missing = len(data.columns) - (data.apply(lambda x: x.count(), axis=1))\n",
    "missing_values_data_rows = pd.DataFrame({'data_missing':data_missing})\n",
    "missing_values_data_rows.sort_values('data_missing',inplace=True,ascending=False)\n",
    "missing_values_data_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now lets analyse the missing data per class (Severity = 0 or Severity = 1).**\n",
    "\n",
    "*First we group the missing values per class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data.groupby('Severity')\n",
    "missing_values_class = grouped_data.count().rsub(grouped_data.size(), axis=0)\n",
    "missing_values_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we split the dataframe per class so we can draw our plot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_new_1, m_new_2 = missing_values_class.head(1), missing_values_class.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(m_new_1.axes[1].to_list()))\n",
    "width = 0.4\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, m_new_1.values[0], width=width, label = \"Severity 0\")\n",
    "rects2 = ax.bar(x + width/2, m_new_2.values[0], width=width, label = \"Severity 1\")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(m_new_1.axes[1].to_list())\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, for each class we're going to calculate the number of rows that have 1 and 2 NaN values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_mv1_sv0 = 0\n",
    "rows_mv2_sv0 = 0\n",
    "rows_mv1_sv1 = 0\n",
    "rows_mv2_sv1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    if(row['Severity'] == 0):\n",
    "        if(row.isnull().sum() == 1):\n",
    "            rows_mv1_sv0 += 1\n",
    "        elif(row.isnull().sum() == 2):\n",
    "            rows_mv2_sv0 += 1\n",
    "    else:\n",
    "        if(row.isnull().sum() == 1):\n",
    "            rows_mv1_sv1 += 1\n",
    "        elif(row.isnull().sum() == 2):\n",
    "            rows_mv2_sv1 += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We create a dataframe only for visualization purpose*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberofnan_class = pd.DataFrame(np.array([[rows_mv1_sv0,rows_mv2_sv0], [rows_mv1_sv1,rows_mv2_sv1]]), \n",
    "                                    index=['Severity 0','Severity 1'], columns=['1 NaN', '2 NaN'])\n",
    "numberofnan_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['1 NaN', '2 NaN']\n",
    "x = np.arange(len(labels))\n",
    "width = 0.4\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, [rows_mv1_sv0,rows_mv2_sv0], width=width, label = \"Severity 0\")\n",
    "rects2 = ax.bar(x + width/2, [rows_mv1_sv1,rows_mv2_sv1], width=width, label = \"Severity 1\")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*With this information we can also see the number of rows with 1 or 2 missing values per class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_class = pd.DataFrame(np.array([[rows_mv1_sv0+rows_mv2_sv0], [rows_mv1_sv1+rows_mv2_sv1]]), \n",
    "                                    index=['Severity 0','Severity 1'], columns=['Sum'])\n",
    "nan_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 5))\n",
    "plt.bar(['Severity 0','Severity 1'],[rows_mv1_sv0+rows_mv2_sv0,rows_mv1_sv1+rows_mv2_sv1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The missing data seems randomly distributed, so we decided to go with the following strategy: **\n",
    "\n",
    "* Drop rows with 2 NaN values\n",
    "\n",
    "* Replace the NaN values from rows with 1 missing value\n",
    "\n",
    "*First we get the mode of every feature for each class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_sv0 = data[data['Severity'] == 0].mode()\n",
    "mode_sv1 = data[data['Severity'] == 1].mode()\n",
    "mode_sv0 = mode_sv0.drop([1])\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(mode_sv0)\n",
    "    print(mode_sv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*After we create conditions to replace the NaN values on rows with 1 missing value.*\n",
    "\n",
    "*For that we need the index of the rows which have 1 missing value.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_1nan = missing_values_data_rows.index[missing_values_data_rows['data_missing'] == 1].tolist()\n",
    "mask_sv0 = (data['Severity'] == 0) & (data.index.isin(rows_1nan))\n",
    "mask_sv1 = (data['Severity'] == 1) & (data.index.isin(rows_1nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can now proceed and replace the missing values for their class mode* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[mask_sv0, 'Shape'] = data.loc[mask_sv0, 'Shape'].fillna(mode_sv0.loc[0,'Shape'])\n",
    "data.loc[mask_sv0, 'Margin'] = data.loc[mask_sv0, 'Margin'].fillna(mode_sv0.loc[0,'Margin'])\n",
    "data.loc[mask_sv0, 'Density'] = data.loc[mask_sv0, 'Density'].fillna(mode_sv0.loc[0,'Density'])\n",
    "data.loc[mask_sv1, 'Age'] = data.loc[mask_sv1, 'Age'].fillna(mode_sv1.loc[0,'Age'])\n",
    "data.loc[mask_sv1, 'Shape'] = data.loc[mask_sv1, 'Shape'].fillna(mode_sv1.loc[0,'Shape'])\n",
    "data.loc[mask_sv1, 'Margin'] = data.loc[mask_sv1, 'Margin'].fillna(mode_sv1.loc[0,'Margin'])\n",
    "data.loc[mask_sv1, 'Density'] = data.loc[mask_sv1, 'Density'].fillna(mode_sv1.loc[0,'Density'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Finally, we can drop rows with NaN values because the only ones that are left are the ones with 2 NaN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data.index = np.arange(1, len(data) + 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "https://towardsdatascience.com/a-starter-pack-to-exploratory-data-analysis-with-python-pandas-seaborn-and-scikit-learn-a77889485baf#249d\n",
    "\n",
    "https://towardsdatascience.com/comprehensive-guide-to-exploratory-data-analysis-of-habermans-survival-data-set-b33f0373c83a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Auxiliar functions & General definitions **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_palette = ['tab:green','tab:red']\n",
    "\n",
    "def categorical_summarized(dataframe, x=None, y=None, hue=None, palette='Set1', verbose=True):\n",
    "    if x == None:\n",
    "        column_interested = y\n",
    "    else:\n",
    "        column_interested = x\n",
    "    series = dataframe[column_interested]\n",
    "    print(series.describe())\n",
    "    print('mode: ', series.mode())\n",
    "    if verbose:\n",
    "        print('='*80)\n",
    "        print(series.value_counts())\n",
    "\n",
    "    sns.countplot(x=x, y=y, hue=hue, data=dataframe, palette=palette)\n",
    "    plt.show()\n",
    "\n",
    "def quantitative_summarized(dataframe, x=None, y=None, hue=None, palette='Set1', ax=None, verbose=True, swarm=False):\n",
    "    series = dataframe[y]\n",
    "    print(series.describe())\n",
    "    print('mode: ', series.mode())\n",
    "    if verbose:\n",
    "        print('='*80)\n",
    "        print(series.value_counts())\n",
    "\n",
    "    sns.boxplot(x=x, y=y, hue=hue, data=dataframe, palette=palette, ax=ax)\n",
    "\n",
    "    if swarm:\n",
    "        sns.swarmplot(x=x, y=y, hue=hue, data=dataframe,\n",
    "                      palette=palette, ax=ax)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Countplot of the Severity (Benign 0 vs Malignant 1) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "ax = sns.countplot(x='Severity',data=data,palette=c_palette)\n",
    "\n",
    "\n",
    "total = len(data['Severity'])\n",
    "\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:.1f}%'.format(100 * height/total),\n",
    "            ha=\"center\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Severity on Age **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantitative_summarized(dataframe= data, y = 'Age', x = 'Severity', palette=c_palette, verbose=False, swarm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.FacetGrid(data, hue='Severity', size=8, palette=c_palette) \\\n",
    "    .map(sns.distplot, 'Age', bins=10) \\\n",
    "    .add_legend()\n",
    "plt.xticks([0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "g = sns.FacetGrid(data,hue='Severity',palette=c_palette,size=6,aspect=2)\n",
    "g = g.map(plt.hist,'Age',bins=20,alpha=0.7).add_legend()\n",
    "plt.xticks([15,18,22,25,28,32,35,38,42,45,48,52,55,59,62,65,69,72,75,79,82,86,89,93,96,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='Age', y='Severity', data=data, kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Severity on Shape (mass shape: round=1 oval=2 lobular=3 irregular=4) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_summarized(data, y = 'Shape', hue='Severity', palette=c_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data = data, x='Severity', y='Shape', palette=c_palette)\n",
    "sns.swarmplot(data = data, x='Severity', y='Shape', color = 'k', alpha = 0.6, palette=c_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Severity on Margin (mass shape: round=1 oval=2 lobular=3 irregular=4) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_summarized(data, y = 'Margin', hue='Severity', palette=c_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data = data, x='Severity', y='Margin', palette=c_palette)\n",
    "sns.swarmplot(data = data, x='Severity', y='Margin', color = 'k', alpha = 0.6, palette=c_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Severity on Density (mass density high=1 iso=2 low=3 fat-containing=4) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_summarized(data, y = 'Density', hue='Severity', palette=c_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data = data, x='Severity', y='Density', palette=c_palette)\n",
    "sns.swarmplot(data = data, x='Severity', y='Density', color = 'k', alpha = 0.6, palette=c_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detect outliers: https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Outliers using Box plot (Uni-variate outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=data['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=data['Shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=data['Margin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=data['Density'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Outliers using Scatter plot (Multi-variate outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.scatter(data['Age'], data['Shape'])\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Shape')\n",
    "#ax.set_ylabel('Margin')\n",
    "#ax.set_ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect outliers using mathematical function Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.abs(stats.zscore(data))\n",
    "threshold = 3\n",
    "print(np.where(z > threshold))\n",
    "# The first array contains the list of row numbers and second array respective column numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column 3 (density) has all outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect outliers using IQR Score\n",
    "Similar to Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "iqr = Q3 - Q1\n",
    "print(iqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não curti ...\n",
    "#print(data < (Q1 - 1.5 * iqr)) |(data > (Q3 + 1.5 * iqr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers using Z-Score\n",
    "\n",
    "##### + explanations: https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-data-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Só fazer 1 vez\n",
    "data = data[(np.abs(stats.zscore(data)) < 3).all(axis=1)]\n",
    "data.index = np.arange(1, len(data) + 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting pandas dataframes to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Severity',axis=1).to_numpy()\n",
    "y = data['Severity'].to_numpy()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the attribute data using StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Fit scaler to the features **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Só fazer 1 vez\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Transform the features to a scaled version **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaler.transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create training and testing sets of the data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks & Genetic Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Função que contrói o modelo ANN\n",
    "def buildModel(hidden_layers, nodes_per_layer, activation_fn, optimizer, loss_fn, metrics, inputs=4,):\n",
    "    model = Sequential()\n",
    "    #add input layer\n",
    "    model.add(Dense(inputs, activation=activation_fn, input_shape=(inputs,)))\n",
    "\n",
    "    #add hidden layers    \n",
    "    for i in range(hidden_layers):\n",
    "        model.add(Dense(nodes_per_layer, activation=activation_fn))\n",
    "\n",
    "    #add output layer\n",
    "    model.add(Dense(1,activation=activation_fn))\n",
    "\n",
    "    #compile model\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "#Função que cria a população inicial\n",
    "#parameters=[hidden_layers,nodes_per_layer,activation_fn]\n",
    "def create_new_population():\n",
    "    \n",
    "    population=[]\n",
    "    \n",
    "    for i in range(10):\n",
    "        cromo=[]\n",
    "        cromo.append(np.random.randint(low=1, high=20))\n",
    "        cromo.append(np.random.choice([1, 2, 4, 8, 16, 32, 64, 128, 256]))\n",
    "        cromo.append(np.random.randint(low=0, high=4))\n",
    "        population.append(cromo)\n",
    "        \n",
    "    return np.array(population)\n",
    "\n",
    "#Função genérica que atualiza os argumentos do classifier\n",
    "#parameters=[hidden_layers,nodes_per_layer,activation_fn]\n",
    "def update_model_parameters(parameters):\n",
    "    \n",
    "    if((parameters[2]) == 0): a_f = 'relu'\n",
    "    if((parameters[2]) == 1): a_f = 'softmax'\n",
    "    if((parameters[2]) == 2): a_f = 'sigmoid'\n",
    "    if((parameters[2]) == 3): a_f = 'tanh'\n",
    "        \n",
    "    model = buildModel(parameters[0],parameters[1],a_f,optimizer='rmsprop', loss_fn='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(type(a_f))\n",
    "    return model\n",
    "\n",
    "def select_mating_pool(pop, fitness, parents_fitness, num_parents):\n",
    "    # Selecting the best individuals in the current generation as parents for producing the offspring of the next generation.\n",
    "    parents = np.empty((num_parents, pop.shape[1]))\n",
    "    #parents_fitness=[] - strangely not working good\n",
    "    for parent_num in range(num_parents):\n",
    "        #save fitness values of best parents\n",
    "        parents_fitness.append(np.max(fitness))\n",
    "        #save best parents\n",
    "        max_fitness_idx = np.where(fitness == np.max(fitness))\n",
    "        max_fitness_idx = max_fitness_idx[0][0]\n",
    "        parents[parent_num, :] = pop[max_fitness_idx, :]\n",
    "        fitness[max_fitness_idx] = -99999999999\n",
    "        \n",
    "    return parents\n",
    "\n",
    "def crossover(parents, offspring_size):\n",
    "    offspring = np.empty(offspring_size)\n",
    "    # The point at which crossover takes place between two parents. Usually it is at the center.\n",
    "    crossover_point = np.uint8(offspring_size[1]/2)\n",
    "\n",
    "    for k in range(offspring_size[0]):\n",
    "        # Index of the first parent to mate.\n",
    "        parent1_idx = k%parents.shape[0]\n",
    "        # Index of the second parent to mate.\n",
    "        parent2_idx = (k+1)%parents.shape[0]\n",
    "        # The new offspring will have its first half of its genes taken from the first parent.\n",
    "        offspring[k, 0:crossover_point] = parents[parent1_idx, 0:crossover_point]\n",
    "        # The new offspring will have its second half of its genes taken from the second parent.\n",
    "        offspring[k, crossover_point:] = parents[parent2_idx, crossover_point:]\n",
    "    return offspring\n",
    "\n",
    "def mutation(offspring_crossover):\n",
    "    # Mutation changes a single gene in each offspring randomly.\n",
    "    for idx in range(offspring_crossover.shape[0]):\n",
    "        \n",
    "        # Select which gene to mutate\n",
    "        select_gene = np.random.randint(low=0, high=3)\n",
    " \n",
    "        if(select_gene == 0):\n",
    "            #num_hidden_layers mutation\n",
    "            random_value = np.random.randint(low=1, high=20)\n",
    "            offspring_crossover[idx,0] = random_value\n",
    "        if(select_gene == 1):\n",
    "            #num_nodes_per_layer mutation\n",
    "            random_value = np.random.choice([1, 2, 4, 8, 16, 32, 64, 128, 256])\n",
    "            offspring_crossover[idx,1] = random_value\n",
    "        if(select_gene == 2):\n",
    "            #activation function mutation\n",
    "            random_value = np.random.randint(low=0, high=4)\n",
    "            offspring_crossover[idx,2] = random_value\n",
    "            \n",
    "    return offspring_crossover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** K-Fold Cross Validation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "num_folds = 10\n",
    "#acc_per_fold = []\n",
    "#loss_per_fold = []\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Keras ANN built from specific parameters (one specific chromosome) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_create_folds(chromosome):\n",
    "    scores=[]\n",
    "    \n",
    "    model = update_model_parameters(chromosome)\n",
    "    \n",
    "    fold_no=1\n",
    "    \n",
    "    for train, test in kfold.split(X, y):\n",
    "\n",
    "        history = model.fit(X[train], y[train],\n",
    "              epochs=20,\n",
    "              batch_size=128,\n",
    "              verbose=0)\n",
    "\n",
    "        score = model.evaluate(X[test], y[test], batch_size=128, verbose=0)\n",
    "\n",
    "        #acc_per_fold.append(score[1] * 100)\n",
    "        #loss_per_fold.append(score[0])\n",
    "        \n",
    "        #Adding all accuracy values to an array\n",
    "        scores.append(score[1])\n",
    "    \n",
    "        print(\"Fold %d: loss = %.2f || accuracy=  %.2f%%\" % (fold_no, score[0], score[1]*100))\n",
    "\n",
    "        fold_no+=1\n",
    "        \n",
    "    return scores\n",
    "\n",
    "    #print('Average scores for all folds:')\n",
    "    #print(\"> Accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(acc_per_fold),np.std(acc_per_fold)))\n",
    "    #print(\"> Loss: %.2f \" % (np.mean(loss_per_fold)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Hyperparameter optimization using Genetic Algorithms **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "population = create_new_population()\n",
    "print(population)\n",
    "\n",
    "num_parents_mating = 5\n",
    "num_generations = 20\n",
    "# number of genes for each chromosome\n",
    "num_genes = 3\n",
    "# number of chromosomes for each population\n",
    "num_chromosomes = 10 \n",
    "\n",
    "pop_size=(num_chromosomes,num_genes)\n",
    "\n",
    "# fitness values for each chromosome for the current generation\n",
    "fitness_values = []\n",
    "# fitness vaalues for each chromosome of the last generation\n",
    "last_fitness_values = []\n",
    "\n",
    "gen = 0\n",
    "cromo = 0\n",
    "\n",
    "parents=[]\n",
    "# Parents fitness so we do not repeat calculations on parents\n",
    "parents_fitness = []\n",
    "\n",
    "performances=[]\n",
    "hiperparameters=[]\n",
    "\n",
    "\n",
    "for generation in range(num_generations):\n",
    "    gen+=1\n",
    "    cromo = 0\n",
    "    best_perf_per_gen = -1\n",
    "    \n",
    "    for chromosome in population:\n",
    "        cromo+=1\n",
    "        score=-1\n",
    "        parentNumber=0\n",
    "        \n",
    "        # If it's a known chromosome we dont need to train the ANN again\n",
    "        # Skips the first generation because we didnt select the parents yet\n",
    "        for savedCromo in parents:\n",
    "            parentNumber+=1\n",
    "            if (np.array_equal(chromosome,savedCromo)):\n",
    "                score = parents_fitness[parentNumber-1]\n",
    "                print(savedCromo, \"Known Chromosome\")\n",
    "                print(score, \"Score Chromosome\")\n",
    "\n",
    "        \n",
    "        # If it's a new chromosome we need to train the ANN in order to get the accuracy\n",
    "        if (score < 0):\n",
    "            scores = classify_create_folds(chromosome)\n",
    "            score = sum(scores)/len(scores)\n",
    "            \n",
    "        print(\"Generation-{}\".format(gen),\"Chromosome-{}\".format(cromo),\"scored\",score)\n",
    "        # Keep the scores in fitness_values\n",
    "        fitness_values.append(score)\n",
    "        \n",
    "        # Getting the best hyperparameters per generation to check the evolution at the end\n",
    "        if(best_perf_per_gen < score):\n",
    "            best_perf_per_gen = score\n",
    "            best_cromo_per_gen = chromosome\n",
    "           \n",
    "        print(chromosome)\n",
    "        \n",
    "    performances.append(best_perf_per_gen)\n",
    "    hiperparameters.append(best_cromo_per_gen)\n",
    "   \n",
    "    \n",
    "    print(performances,\"Best accuracies of each generation\")\n",
    "    print(hiperparameters,\"Best of each generation\")\n",
    "    \n",
    "    # We store last generation in other array because fitness_values is changed by the selec_mating_pool\n",
    "    if(gen == num_generations):\n",
    "        for i in fitness_values:\n",
    "            last_fitness_values.append(i)\n",
    "            \n",
    "    print(last_fitness_values,\"Last Fitness Values\")\n",
    "    \n",
    "    parents_fitness = []\n",
    "    parents = select_mating_pool(population,fitness_values,parents_fitness,num_parents_mating)\n",
    "\n",
    "    # Generating next generation using crossover.\n",
    "    offspring_crossover = crossover(parents,\n",
    "                                        offspring_size=(pop_size[0]-parents.shape[0], num_genes))\n",
    "\n",
    "\n",
    "    # Adding some variations to the offspring using mutation.\n",
    "    offspring_mutation = mutation(offspring_crossover)\n",
    "\n",
    "    # Creating the new population based on the parents and offspring.\n",
    "    population[0:parents.shape[0], :] = parents\n",
    "    population[parents.shape[0]:, :] = offspring_mutation\n",
    "    \n",
    "    # Reset fitness_values\n",
    "    fitness_values=[]\n",
    "\n",
    "# Getting the best solution\n",
    "print(population)\n",
    "best_solution = population[last_fitness_values.index(np.max(last_fitness_values))]\n",
    "print(\"The best hyperparameters obtained are\",best_solution,\"with an accuracy of\",np.max(last_fitness_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "testModel = buildModel(hidden_layers=3, nodes_per_layer=32, activation_fn='relu',  optimizer='rmsprop', loss_fn='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = testModel.fit(X_train, y_train,\n",
    "          #epochs=20,\n",
    "          #batch_size=128, validation_split=0.2)\n",
    "\n",
    "score = testModel.evaluate(X_train, y_train, batch_size=128, verbose=1)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
    "\n",
    "# Visualize history\n",
    "# Plot history: Loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Validation loss history')\n",
    "plt.ylabel('Loss value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.show()\n",
    "\n",
    "# Plot history: Accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Validation accuracy history')\n",
    "plt.ylabel('Accuracy value (%)')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "num_folds = 10\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "fold_no=1\n",
    "for train, test in kfold.split(X, y):\n",
    "    testModel = buildModel(hidden_layers=3, nodes_per_layer=32, activation_fn='relu',  optimizer='rmsprop', loss_fn='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = testModel.fit(X[train], y[train],\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          verbose=0)\n",
    "\n",
    "    score = testModel.evaluate(X[test], y[test], batch_size=128, verbose=0)\n",
    "\n",
    "    \n",
    "    acc_per_fold.append(score[1] * 100)\n",
    "    loss_per_fold.append(score[0])\n",
    "    \n",
    "    print(\"Fold %d: loss = %.2f || accuracy=  %.2f%%\" % (fold_no, score[0], score[1]*100))\n",
    "\n",
    "    fold_no+=1\n",
    "\n",
    "print('Average scores for all folds:')\n",
    "print(\"> Accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(acc_per_fold),np.std(acc_per_fold)))\n",
    "print(\"> Loss: %.2f \" % (np.mean(loss_per_fold)))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('ClinicalExaminations': pipenv)",
   "language": "python",
   "name": "python37664bitclinicalexaminationspipenvad0378402ce04cb7908ba7c074f33903"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
