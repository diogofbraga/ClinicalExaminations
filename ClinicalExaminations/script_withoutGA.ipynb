{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP - Computação Natural\n",
    "#### \"Predict whether a mammogram mass is benign or malignant\"\n",
    "\n",
    "1. BI-RADS assessment: 1 to 5 (ordinal)  \n",
    "2. Age: patient's age in years (integer)\n",
    "3. Shape: mass shape: round=1 oval=2 lobular=3 irregular=4 (nominal)\n",
    "4. Margin: mass margin: circumscribed=1 microlobulated=2 obscured=3 ill-defined=4 spiculated=5 (nominal)\n",
    "5. Density: mass density high=1 iso=2 low=3 fat-containing=4 (ordinal)\n",
    "6. Severity: benign=0 or malignant=1 (binominal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mammographic_masses.data.txt')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Convert missing data (indicated by a ?) into NaN and add the appropriate column names (BI_RADS, age, shape, margin, density, and severity) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace('?',np.nan)\n",
    "data.columns = ['BI_RADS','Age','Shape','Margin','Density','Severity']\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Drop BI_RADS column because it has no influence on the severity forecast **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['BI_RADS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Convert datatype 'object' to 'float64' **  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(float)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing missing values\n",
    "**First we get the missing values per feature.**\n",
    "\n",
    "*Lets check them out as well*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_feature = data.isnull().sum(axis=0)\n",
    "graph = missing_values_feature.drop(labels='Severity')\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(131)\n",
    "plt.bar(graph.axes[0].to_list(), graph.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We then develop a heatmap to give us some more information*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data.isnull(),yticklabels=False,cbar=False,cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Finally we can check the percentage of missing values per feature*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing = data.isnull().sum() * 100 / len(data)\n",
    "missing_value_df = pd.DataFrame({'percent_missing': percent_missing})\n",
    "print(missing_value_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After analysing the columns, we should have a look at the rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_missing = len(data.columns) - (data.apply(lambda x: x.count(), axis=1))\n",
    "missing_values_data_rows = pd.DataFrame({'data_missing':data_missing})\n",
    "missing_values_data_rows.sort_values('data_missing',inplace=True,ascending=False)\n",
    "missing_values_data_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now lets analyse the missing data per class (Severity = 0 or Severity = 1).**\n",
    "\n",
    "*First we group the missing values per class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data.groupby('Severity')\n",
    "missing_values_class = grouped_data.count().rsub(grouped_data.size(), axis=0)\n",
    "missing_values_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we split the dataframe per class so we can draw our plot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_new_1, m_new_2 = missing_values_class.head(1), missing_values_class.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(m_new_1.axes[1].to_list()))\n",
    "width = 0.4\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, m_new_1.values[0], width=width, label = \"Severity 0\")\n",
    "rects2 = ax.bar(x + width/2, m_new_2.values[0], width=width, label = \"Severity 1\")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(m_new_1.axes[1].to_list())\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, for each class we're going to calculate the number of rows that have 1 and 2 NaN values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_mv1_sv0 = 0\n",
    "rows_mv2_sv0 = 0\n",
    "rows_mv1_sv1 = 0\n",
    "rows_mv2_sv1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    if(row['Severity'] == 0):\n",
    "        if(row.isnull().sum() == 1):\n",
    "            rows_mv1_sv0 += 1\n",
    "        elif(row.isnull().sum() == 2):\n",
    "            rows_mv2_sv0 += 1\n",
    "    else:\n",
    "        if(row.isnull().sum() == 1):\n",
    "            rows_mv1_sv1 += 1\n",
    "        elif(row.isnull().sum() == 2):\n",
    "            rows_mv2_sv1 += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We create a dataframe only for visualization purpose*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberofnan_class = pd.DataFrame(np.array([[rows_mv1_sv0,rows_mv2_sv0], [rows_mv1_sv1,rows_mv2_sv1]]), \n",
    "                                    index=['Severity 0','Severity 1'], columns=['1 NaN', '2 NaN'])\n",
    "numberofnan_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['1 NaN', '2 NaN']\n",
    "x = np.arange(len(labels))\n",
    "width = 0.4\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, [rows_mv1_sv0,rows_mv2_sv0], width=width, label = \"Severity 0\")\n",
    "rects2 = ax.bar(x + width/2, [rows_mv1_sv1,rows_mv2_sv1], width=width, label = \"Severity 1\")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*With this information we can also see the number of rows with 1 or 2 missing values per class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_class = pd.DataFrame(np.array([[rows_mv1_sv0+rows_mv2_sv0], [rows_mv1_sv1+rows_mv2_sv1]]), \n",
    "                                    index=['Severity 0','Severity 1'], columns=['Sum'])\n",
    "nan_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 5))\n",
    "plt.bar(['Severity 0','Severity 1'],[rows_mv1_sv0+rows_mv2_sv0,rows_mv1_sv1+rows_mv2_sv1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The missing data seems randomly distributed, so we decided to go with the following strategy: **\n",
    "\n",
    "* Drop rows with 2 NaN values\n",
    "\n",
    "* Replace the NaN values from rows with 1 missing value\n",
    "\n",
    "*First we get the mode of every feature for each class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_sv0 = data[data['Severity'] == 0].mode()\n",
    "mode_sv1 = data[data['Severity'] == 1].mode()\n",
    "mode_sv0 = mode_sv0.drop([1])\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(mode_sv0)\n",
    "    print(mode_sv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*After we create conditions to replace the NaN values on rows with 1 missing value.*\n",
    "\n",
    "*For that we need the index of the rows which have 1 missing value.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_1nan = missing_values_data_rows.index[missing_values_data_rows['data_missing'] == 1].tolist()\n",
    "mask_sv0 = (data['Severity'] == 0) & (data.index.isin(rows_1nan))\n",
    "mask_sv1 = (data['Severity'] == 1) & (data.index.isin(rows_1nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can now proceed and replace the missing values for their class mode* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[mask_sv0, 'Shape'] = data.loc[mask_sv0, 'Shape'].fillna(mode_sv0.loc[0,'Shape'])\n",
    "data.loc[mask_sv0, 'Margin'] = data.loc[mask_sv0, 'Margin'].fillna(mode_sv0.loc[0,'Margin'])\n",
    "data.loc[mask_sv0, 'Density'] = data.loc[mask_sv0, 'Density'].fillna(mode_sv0.loc[0,'Density'])\n",
    "data.loc[mask_sv1, 'Age'] = data.loc[mask_sv1, 'Age'].fillna(mode_sv1.loc[0,'Age'])\n",
    "data.loc[mask_sv1, 'Shape'] = data.loc[mask_sv1, 'Shape'].fillna(mode_sv1.loc[0,'Shape'])\n",
    "data.loc[mask_sv1, 'Margin'] = data.loc[mask_sv1, 'Margin'].fillna(mode_sv1.loc[0,'Margin'])\n",
    "data.loc[mask_sv1, 'Density'] = data.loc[mask_sv1, 'Density'].fillna(mode_sv1.loc[0,'Density'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Finally, we can drop rows with NaN values because the only ones that are left are the ones with 2 NaN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data.index = np.arange(1, len(data) + 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "https://towardsdatascience.com/a-starter-pack-to-exploratory-data-analysis-with-python-pandas-seaborn-and-scikit-learn-a77889485baf#249d\n",
    "\n",
    "https://towardsdatascience.com/comprehensive-guide-to-exploratory-data-analysis-of-habermans-survival-data-set-b33f0373c83a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Auxiliar functions & General definitions **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_palette = ['tab:green','tab:red']\n",
    "\n",
    "def categorical_summarized(dataframe, x=None, y=None, hue=None, palette='Set1', verbose=True):\n",
    "    if x == None:\n",
    "        column_interested = y\n",
    "    else:\n",
    "        column_interested = x\n",
    "    series = dataframe[column_interested]\n",
    "    print(series.describe())\n",
    "    print('mode: ', series.mode())\n",
    "    if verbose:\n",
    "        print('='*80)\n",
    "        print(series.value_counts())\n",
    "\n",
    "    sns.countplot(x=x, y=y, hue=hue, data=dataframe, palette=palette)\n",
    "    plt.show()\n",
    "\n",
    "def quantitative_summarized(dataframe, x=None, y=None, hue=None, palette='Set1', ax=None, verbose=True, swarm=False):\n",
    "    series = dataframe[y]\n",
    "    print(series.describe())\n",
    "    print('mode: ', series.mode())\n",
    "    if verbose:\n",
    "        print('='*80)\n",
    "        print(series.value_counts())\n",
    "\n",
    "    sns.boxplot(x=x, y=y, hue=hue, data=dataframe, palette=palette, ax=ax)\n",
    "\n",
    "    if swarm:\n",
    "        sns.swarmplot(x=x, y=y, hue=hue, data=dataframe,\n",
    "                      palette=palette, ax=ax)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Countplot of the Severity (Benign 0 vs Malignant 1) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "ax = sns.countplot(x='Severity',data=data,palette=c_palette)\n",
    "\n",
    "\n",
    "total = len(data['Severity'])\n",
    "\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:.1f}%'.format(100 * height/total),\n",
    "            ha=\"center\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Severity on Age **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantitative_summarized(dataframe= data, y = 'Age', x = 'Severity', palette=c_palette, verbose=False, swarm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.FacetGrid(data, hue='Severity', size=8, palette=c_palette) \\\n",
    "    .map(sns.distplot, 'Age', bins=10) \\\n",
    "    .add_legend()\n",
    "plt.xticks([0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "g = sns.FacetGrid(data,hue='Severity',palette=c_palette,size=6,aspect=2)\n",
    "g = g.map(plt.hist,'Age',bins=20,alpha=0.7).add_legend()\n",
    "plt.xticks([15,18,22,25,28,32,35,38,42,45,48,52,55,59,62,65,69,72,75,79,82,86,89,93,96,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='Age', y='Severity', data=data, kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Severity on Shape (mass shape: round=1 oval=2 lobular=3 irregular=4) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_summarized(data, y = 'Shape', hue='Severity', palette=c_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data = data, x='Severity', y='Shape', palette=c_palette)\n",
    "sns.swarmplot(data = data, x='Severity', y='Shape', color = 'k', alpha = 0.6, palette=c_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Severity on Margin (mass shape: round=1 oval=2 lobular=3 irregular=4) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_summarized(data, y = 'Margin', hue='Severity', palette=c_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data = data, x='Severity', y='Margin', palette=c_palette)\n",
    "sns.swarmplot(data = data, x='Severity', y='Margin', color = 'k', alpha = 0.6, palette=c_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Severity on Density (mass density high=1 iso=2 low=3 fat-containing=4) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_summarized(data, y = 'Density', hue='Severity', palette=c_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data = data, x='Severity', y='Density', palette=c_palette)\n",
    "sns.swarmplot(data = data, x='Severity', y='Density', color = 'k', alpha = 0.6, palette=c_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detect outliers: https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Outliers using Box plot (Uni-variate outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=data['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=data['Shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=data['Margin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=data['Density'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Outliers using Scatter plot (Multi-variate outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.scatter(data['Age'], data['Shape'])\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Shape')\n",
    "#ax.set_ylabel('Margin')\n",
    "#ax.set_ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect outliers using mathematical function Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.abs(stats.zscore(data))\n",
    "threshold = 3\n",
    "print(np.where(z > threshold))\n",
    "# The first array contains the list of row numbers and second array respective column numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column 3 (density) has all outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect outliers using IQR Score\n",
    "Similar to Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "iqr = Q3 - Q1\n",
    "print(iqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não curti ...\n",
    "#print(data < (Q1 - 1.5 * iqr)) |(data > (Q3 + 1.5 * iqr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers using Z-Score\n",
    "\n",
    "##### + explanations: https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-data-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Só fazer 1 vez\n",
    "data = data[(np.abs(stats.zscore(data)) < 3).all(axis=1)]\n",
    "data.index = np.arange(1, len(data) + 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting pandas dataframes to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Severity',axis=1).to_numpy()\n",
    "y = data['Severity'].to_numpy()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the attribute data using StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Fit scaler to the features **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Só fazer 1 vez\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Transform the features to a scaled version **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaler.transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "y = y.reshape(len(y), 1)\n",
    "y_encoded = onehot_encoder.fit_transform(y)\n",
    "\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "y_train_encoded = onehot_encoder.fit_transform(y_train)\n",
    "\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "y_test_encoded = onehot_encoder.fit_transform(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def buildModel(hidden_layers, nodes_per_layer, activation_fn, optimizer, loss_fn, metrics, inputs=4,):\n",
    "    model = Sequential()\n",
    "    #add input layer\n",
    "    model.add(Dense(inputs, activation=activation_fn, input_shape=(inputs,)))\n",
    "\n",
    "    #add hidden layers    \n",
    "    for i in range(hidden_layers):\n",
    "        model.add(Dense(nodes_per_layer, activation=activation_fn))\n",
    "\n",
    "    #add output layer\n",
    "    model.add(Dense(2,activation=activation_fn))\n",
    "\n",
    "    #compile model\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testModel = buildModel(hidden_layers=3, nodes_per_layer=32, activation_fn='relu',  optimizer='rmsprop', loss_fn='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = testModel.fit(X_train, y_train_encoded,\n",
    "          epochs=50,\n",
    "          batch_size=128, validation_split=0.2)\n",
    "\n",
    "score = testModel.evaluate(X_test, y_test_encoded, batch_size=128, verbose=1)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
    "\n",
    "# Visualize history\n",
    "# Plot history: Loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Validation loss history')\n",
    "plt.ylabel('Loss value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.show()\n",
    "\n",
    "# Plot history: Accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Validation accuracy history')\n",
    "plt.ylabel('Accuracy value (%)')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "num_folds = 10\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "scores=[]\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "fold_no=1\n",
    "for train, test in kfold.split(X, y_encoded):\n",
    "    testModel = buildModel(hidden_layers=3, nodes_per_layer=20, activation_fn='relu',  optimizer='rmsprop', loss_fn='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = testModel.fit(X[train], y_encoded[train],\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          verbose=0)\n",
    "\n",
    "    score = testModel.evaluate(X[test], y_encoded[test], batch_size=128, verbose=0)\n",
    "    \n",
    "    acc_per_fold.append(score[1] * 100)\n",
    "    loss_per_fold.append(score[0])\n",
    "    \n",
    "    print(\"Fold %d: loss = %.2f || accuracy=  %.2f%%\" % (fold_no, score[0], score[1]*100))\n",
    "\n",
    "    fold_no+=1\n",
    "\n",
    "print('Average scores for all folds:')\n",
    "print(\"> Accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(acc_per_fold),np.std(acc_per_fold)))\n",
    "print(\"> Loss: %.2f \" % (np.mean(loss_per_fold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from keras.utils import to_categorical\n",
    "y_binary = to_categorical(y_int)\n",
    "\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "\n",
    "# creating model\n",
    "inputs = Input(shape = (4,))\n",
    "dense1 = Dense(8, activation = 'relu')(inputs)\n",
    "dense2 = Dense(16, activation = 'relu')(dense1)\n",
    "dense3 = Dense(32, activation = 'relu')(dense2)\n",
    "\n",
    " \n",
    "# use output from dense layer 2 to create autoencder output\n",
    "up_dense1 = Dense(1, activation = 'relu')(dense2)\n",
    "up_dense2 = Dense(1, activation = 'relu')(dense2)\n",
    "\n",
    "model = Model(inputs, [up_dense1,up_dense2])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "m = 256\n",
    "n_epoch = 25\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', loss_weights = [1.0, 0.5], metrics = ['accuracy'])\n",
    "model.fit(X_train,[y_train, y_train], epochs=n_epoch, batch_size=m, shuffle=True)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('ClinicalExaminations': pipenv)",
   "language": "python",
   "name": "python37664bitclinicalexaminationspipenvad0378402ce04cb7908ba7c074f33903"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
