{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP - Computação Natural\n",
    "#### \"Predict whether a mammogram mass is benign or malignant\"\n",
    "\n",
    "1. BI-RADS assessment: 1 to 5 (ordinal)  \n",
    "2. Age: patient's age in years (integer)\n",
    "3. Shape: mass shape: round=1 oval=2 lobular=3 irregular=4 (nominal)\n",
    "4. Margin: mass margin: circumscribed=1 microlobulated=2 obscured=3 ill-defined=4 spiculated=5 (nominal)\n",
    "5. Density: mass density high=1 iso=2 low=3 fat-containing=4 (ordinal)\n",
    "6. Severity: benign=0 or malignant=1 (binominal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy import stats\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD, RMSprop, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mammographic_masses.data.txt')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Convert missing data (indicated by a ?) into NaN and add the appropriate column names (BI_RADS, age, shape, margin, density, and severity) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace('?',np.nan)\n",
    "data.columns = ['BI_RADS','Age','Shape','Margin','Density','Severity']\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Drop BI_RADS column because it has no influence on the severity forecast **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['BI_RADS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Convert datatype 'object' to 'float64' **  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(float)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing missing values\n",
    "**First we get the missing values per feature.**\n",
    "\n",
    "*Lets check them out as well*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_feature = data.isnull().sum(axis=0)\n",
    "graph = missing_values_feature.drop(labels='Severity')\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(131)\n",
    "plt.bar(graph.axes[0].to_list(), graph.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We then develop a heatmap to give us some more information*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data.isnull(),yticklabels=False,cbar=False,cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Finally we can check the percentage of missing values per feature*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing = data.isnull().sum() * 100 / len(data)\n",
    "missing_value_df = pd.DataFrame({'percent_missing': percent_missing})\n",
    "print(missing_value_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After analysing the columns, we should have a look at the rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_missing = len(data.columns) - (data.apply(lambda x: x.count(), axis=1))\n",
    "missing_values_data_rows = pd.DataFrame({'data_missing':data_missing})\n",
    "missing_values_data_rows.sort_values('data_missing',inplace=True,ascending=False)\n",
    "missing_values_data_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now lets analyse the missing data per class (Severity = 0 or Severity = 1).**\n",
    "\n",
    "*First we group the missing values per class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data.groupby('Severity')\n",
    "missing_values_class = grouped_data.count().rsub(grouped_data.size(), axis=0)\n",
    "missing_values_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we split the dataframe per class so we can draw our plot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_new_1, m_new_2 = missing_values_class.head(1), missing_values_class.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(m_new_1.axes[1].to_list()))\n",
    "width = 0.4\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, m_new_1.values[0], width=width, label = \"Severity 0\")\n",
    "rects2 = ax.bar(x + width/2, m_new_2.values[0], width=width, label = \"Severity 1\")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(m_new_1.axes[1].to_list())\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, for each class we're going to calculate the number of rows that have 1 and 2 NaN values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_mv1_sv0 = 0\n",
    "rows_mv2_sv0 = 0\n",
    "rows_mv1_sv1 = 0\n",
    "rows_mv2_sv1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    if(row['Severity'] == 0):\n",
    "        if(row.isnull().sum() == 1):\n",
    "            rows_mv1_sv0 += 1\n",
    "        elif(row.isnull().sum() == 2):\n",
    "            rows_mv2_sv0 += 1\n",
    "    else:\n",
    "        if(row.isnull().sum() == 1):\n",
    "            rows_mv1_sv1 += 1\n",
    "        elif(row.isnull().sum() == 2):\n",
    "            rows_mv2_sv1 += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We create a dataframe only for visualization purpose*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberofnan_class = pd.DataFrame(np.array([[rows_mv1_sv0,rows_mv2_sv0], [rows_mv1_sv1,rows_mv2_sv1]]), \n",
    "                                    index=['Severity 0','Severity 1'], columns=['1 NaN', '2 NaN'])\n",
    "numberofnan_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['1 NaN', '2 NaN']\n",
    "x = np.arange(len(labels))\n",
    "width = 0.4\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, [rows_mv1_sv0,rows_mv2_sv0], width=width, label = \"Severity 0\")\n",
    "rects2 = ax.bar(x + width/2, [rows_mv1_sv1,rows_mv2_sv1], width=width, label = \"Severity 1\")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*With this information we can also see the number of rows with 1 or 2 missing values per class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_class = pd.DataFrame(np.array([[rows_mv1_sv0+rows_mv2_sv0], [rows_mv1_sv1+rows_mv2_sv1]]), \n",
    "                                    index=['Severity 0','Severity 1'], columns=['Sum'])\n",
    "nan_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 5))\n",
    "plt.bar(['Severity 0','Severity 1'],[rows_mv1_sv0+rows_mv2_sv0,rows_mv1_sv1+rows_mv2_sv1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The missing data seems randomly distributed, so we decided to go with the following strategy: **\n",
    "\n",
    "* Drop rows with 2 NaN values\n",
    "\n",
    "* Replace the NaN values from rows with 1 missing value\n",
    "\n",
    "*First we get the mode of every feature for each class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_sv0 = data[data['Severity'] == 0].mode()\n",
    "mode_sv1 = data[data['Severity'] == 1].mode()\n",
    "mode_sv0 = mode_sv0.drop([1])\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(mode_sv0)\n",
    "    print(mode_sv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*After we create conditions to replace the NaN values on rows with 1 missing value.*\n",
    "\n",
    "*For that we need the index of the rows which have 1 missing value.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_1nan = missing_values_data_rows.index[missing_values_data_rows['data_missing'] == 1].tolist()\n",
    "mask_sv0 = (data['Severity'] == 0) & (data.index.isin(rows_1nan))\n",
    "mask_sv1 = (data['Severity'] == 1) & (data.index.isin(rows_1nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can now proceed and replace the missing values for their class mode* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[mask_sv0, 'Shape'] = data.loc[mask_sv0, 'Shape'].fillna(mode_sv0.loc[0,'Shape'])\n",
    "data.loc[mask_sv0, 'Margin'] = data.loc[mask_sv0, 'Margin'].fillna(mode_sv0.loc[0,'Margin'])\n",
    "data.loc[mask_sv0, 'Density'] = data.loc[mask_sv0, 'Density'].fillna(mode_sv0.loc[0,'Density'])\n",
    "data.loc[mask_sv1, 'Age'] = data.loc[mask_sv1, 'Age'].fillna(mode_sv1.loc[0,'Age'])\n",
    "data.loc[mask_sv1, 'Shape'] = data.loc[mask_sv1, 'Shape'].fillna(mode_sv1.loc[0,'Shape'])\n",
    "data.loc[mask_sv1, 'Margin'] = data.loc[mask_sv1, 'Margin'].fillna(mode_sv1.loc[0,'Margin'])\n",
    "data.loc[mask_sv1, 'Density'] = data.loc[mask_sv1, 'Density'].fillna(mode_sv1.loc[0,'Density'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Finally, we can drop rows with NaN values because the only ones that are left are the ones with 2 NaN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data.index = np.arange(1, len(data) + 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Outliers using Box plot (Uni-variate outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect outliers using mathematical function Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.abs(stats.zscore(data))\n",
    "threshold = 3\n",
    "print(np.where(z > threshold))\n",
    "# The first array contains the list of row numbers and second array respective column numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column 3 (density) has all outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect outliers using IQR Score\n",
    "Similar to Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "iqr = Q3 - Q1\n",
    "print(iqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não curti ...\n",
    "#print(data < (Q1 - 1.5 * iqr)) |(data > (Q3 + 1.5 * iqr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers using Z-Score\n",
    "\n",
    "##### + explanations: https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-data-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Só fazer 1 vez\n",
    "data = data[(np.abs(stats.zscore(data)) < 3).all(axis=1)]\n",
    "data.index = np.arange(1, len(data) + 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting pandas dataframes to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = data.drop('Severity',axis=1).to_numpy()\n",
    "#y = data['Severity'].to_numpy()\n",
    "#X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the attribute data using StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Fit scaler to the features **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Só fazer 1 vez\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(X)\n",
    "#scaler.mean_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Transform the features to a scaled version **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X = scaler.transform(X)\n",
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "col_names = ['Age']\n",
    "features = data[col_names]\n",
    "scaler = StandardScaler().fit(features.values)\n",
    "features = scaler.transform(features.values)\n",
    "data[col_names] = features\n",
    "\n",
    "one_hot = pd.get_dummies(data['Shape'])\n",
    "data = data.drop('Shape',axis = 1)\n",
    "data = data.join(one_hot)\n",
    "data = data.rename(columns={1.0: \"Shape_1\", 2.0: \"Shape_2\", 3.0: \"Shape_3\", 4.0: \"Shape_4\"})\n",
    "\n",
    "one_hot = pd.get_dummies(data['Margin'])\n",
    "data = data.drop('Margin',axis = 1)\n",
    "data = data.join(one_hot)\n",
    "data = data.rename(columns={1.0: \"Margin_1\", 2.0: \"Margin_2\", 3.0: \"Margin_3\", 4.0: \"Margin_4\", 5.0: \"Margin_5\"})\n",
    "\n",
    "one_hot = pd.get_dummies(data['Density'])\n",
    "data = data.drop('Density',axis = 1)\n",
    "data = data.join(one_hot)\n",
    "data = data.rename(columns={1.0: \"Density_1\", 2.0: \"Density_2\", 3.0: \"Density_3\", 4.0: \"Density_4\"})\n",
    "\n",
    "one_hot = pd.get_dummies(data['Severity'])\n",
    "data = data.drop('Severity',axis = 1)\n",
    "data = data.join(one_hot)\n",
    "data = data.rename(columns={0.0: \"Severity_0\", 1.0: \"Severity_1\"})\n",
    "\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = data.drop(['Severity_0','Severity_1'],axis=1).to_numpy()\n",
    "y = data[['Severity_0','Severity_1']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.info(X)\n",
    "print(\"---\")\n",
    "np.info(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.info(X_train)\n",
    "print(\"---\")\n",
    "np.info(X_test)\n",
    "print(\"---\")\n",
    "np.info(y_train)\n",
    "print(\"---\")\n",
    "np.info(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(hidden_layers, nodes_per_layer, activation_fn, optimizer, loss_fn, metrics, inputs=13,):\n",
    "    model = Sequential()\n",
    "    #add input layer\n",
    "    model.add(Dense(inputs, activation=activation_fn, input_shape=(inputs,)))\n",
    "\n",
    "    #add hidden layers    \n",
    "    for i in range(hidden_layers):\n",
    "        model.add(Dense(nodes_per_layer, activation=activation_fn))\n",
    "\n",
    "    #add output layer\n",
    "    model.add(Dense(2,activation=activation_fn))\n",
    "    \n",
    "    if(optimizer=='rmsprop'): optimizer = RMSprop(learning_rate=0.1)\n",
    "\n",
    "    #compile model\n",
    "    model.compile(optimizer, loss=loss_fn, metrics=metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "def predictionsModel(model, input_attributes, output_attributes):\n",
    "    print(\"\\n###########predict###########\\n\")\n",
    "    previsoes = model.predict(input_attributes)\n",
    "    \n",
    "    # arredondar para 0 ou 1 pois pretende-se um output binário\n",
    "    LP = []\n",
    "    \n",
    "    f = lambda x: int(round(x))\n",
    "    vfunc = np.vectorize(f)\n",
    "    \n",
    "    for prev in previsoes:\n",
    "        prev = vfunc(prev)\n",
    "        LP.append(prev)\n",
    "        \n",
    "    for i in range(len(output_attributes)):\n",
    "        print(\"Target:\", output_attributes[i], \" Prediction:\", LP[i])\n",
    "        if i > 30:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testModel = buildModel(hidden_layers=3, nodes_per_layer=16, activation_fn='softmax',  optimizer='rmsprop', loss_fn='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = testModel.fit(X_train, y_train, validation_split=0.33, epochs=50, batch_size=64, verbose=2)\n",
    "\n",
    "\n",
    "# Visualize history\n",
    "# Plot history: Loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Validation loss history')\n",
    "plt.ylabel('Loss value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot history: Accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Validation accuracy history')\n",
    "plt.ylabel('Accuracy value (%)')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"###########evaluate###########\\n\")\n",
    "scores = testModel.evaluate(X_test, y_test, batch_size=64, verbose=1)\n",
    "print(\"\\n metrica: %s: %.2f%%\\n\" % (testModel.metrics_names[1], scores[1]*100))\n",
    "\n",
    "\n",
    "predictions = predictionsModel(testModel, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "scores=[]\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "fold_no=1\n",
    "\n",
    "for train, test in kfold.split(X, y):\n",
    "    testModel = buildModel(hidden_layers=3, nodes_per_layer=16, activation_fn='softmax',  optimizer='rmsprop', loss_fn='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = testModel.fit(X[train], y[train],\n",
    "          epochs=50,\n",
    "          batch_size=64,\n",
    "          verbose=0)\n",
    "\n",
    "    score = testModel.evaluate(X[test], y[test], batch_size=64, verbose=0)\n",
    "    \n",
    "    predictions = predictionsModel(testModel, X[test], y[test])\n",
    "    \n",
    "    acc_per_fold.append(score[1] * 100)\n",
    "    loss_per_fold.append(score[0])\n",
    "    \n",
    "    print(\"Fold %d: loss = %.2f || accuracy=  %.2f%%\" % (fold_no, score[0], score[1]*100))\n",
    "\n",
    "    fold_no+=1\n",
    "\n",
    "print('Average scores for all folds:')\n",
    "print(\"> Accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(acc_per_fold),np.std(acc_per_fold)))\n",
    "print(\"> Loss: %.2f \" % (np.mean(loss_per_fold)))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('ClinicalExaminations': pipenv)",
   "language": "python",
   "name": "python37664bitclinicalexaminationspipenvad0378402ce04cb7908ba7c074f33903"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
